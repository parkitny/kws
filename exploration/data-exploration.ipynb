{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81cffe0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5ae15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Download validation set only (because it's smaller) to inspect the data.\n",
    "Path('../data/raw').mkdir(parents=True, exist_ok=True)\n",
    "val_set = torchaudio.datasets.SPEECHCOMMANDS('../data/raw', subset=\"validation\", download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb9741",
   "metadata": {},
   "source": [
    "# Playback + associated STFT spectrogram for some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24532de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playback controls followed by associated spectrogram.\n",
    "# NOTE: MFCC on 16kHz can give empty mel filterbanks (zero values)\n",
    "# downsampling the audio to 8kHz avoids this.\n",
    "\n",
    "# Re-run this cell for new samples.\n",
    "def inspect_samples(dataset, n_samples, transform, is_random=True):\n",
    "    ds_length = len(dataset)\n",
    "    for idx in range(n_samples):\n",
    "        _idx = idx\n",
    "        if is_random:\n",
    "            _idx = int(random.random() * ds_length)\n",
    "        waveform, sample_rate, label, speaker_id, utterance_id = dataset[_idx]\n",
    "        sound = ipd.Audio(waveform, rate=sample_rate)\n",
    "        spec = torch.moveaxis(transform(waveform), 0, -1)\n",
    "        print(f\"Spec - min {spec.min():.2f} max {spec.max():.2f}\")\n",
    "        _spec = spec.detach().cpu().numpy()\n",
    "        _,ret = cv2.imencode('.jpg', _spec) \n",
    "        img = ipd.Image(data=ret)\n",
    "        ipd.display(sound)\n",
    "        ipd.display(img)\n",
    "\n",
    "transform = torchaudio.transforms.Spectrogram(n_fft=256)\n",
    "inspect_samples(dataset=val_set, n_samples=10, transform=transform, is_random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456e1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe some audio samples not the same length. \n",
    "# That is OK, we can still re-size the spectrogram\n",
    "# to the same size, the frequency dimension stays the same\n",
    "# but the time dimension stretches.\n",
    "\n",
    "# The result is either audio slightly sped up / slowed down.\n",
    "# Doing this can be considered a form of augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e2ae20",
   "metadata": {},
   "source": [
    "# Playback + associated MFCC spectrogram for some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86456625",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchaudio.transforms.MFCC(n_mfcc=10)\n",
    "inspect_samples(dataset=val_set, n_samples=10, transform=transform, is_random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b839418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE setting n_mfcc too high can result in zero. It's a result\n",
    "# of nyquist sampling theorem. But more detail here:\n",
    "# https://stackoverflow.com/a/56930624/1897312\n",
    "# Two options: \n",
    "# 1 - reduce n_mfcc if you want to keep sample rate the same.\n",
    "# 2 - downsample the audio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c3bcc",
   "metadata": {},
   "source": [
    "# Display the UMAP manifold for random samples from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a5745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import random\n",
    "import numpy as np\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "def compute_manifold(dataset, n_samples, transform, is_random=False):\n",
    "    X = []\n",
    "    y = []\n",
    "    label_set = set([])\n",
    "    ds_length = len(dataset)\n",
    "    for idx in range(n_samples):\n",
    "        _idx = idx\n",
    "        if is_random:\n",
    "            _idx = int(random.random() * ds_length)\n",
    "        waveform, sample_rate, label, speaker_id, utterance_id = dataset[_idx]\n",
    "        label_set.add(label)\n",
    "        sound = ipd.Audio(waveform, rate=sample_rate)\n",
    "        spec = torch.moveaxis(transform(waveform), 0, -1)\n",
    "        _spec = spec.detach().cpu().numpy()\n",
    "        X.append(Resize((32,32))(spec).flatten())\n",
    "        y.append(list(label_set).index(label))\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.array(y)\n",
    "    manifold = umap.UMAP().fit(X, y)\n",
    "    X_reduced = manifold.transform(X)\n",
    "    label_list = list(label_set)\n",
    "    # Map the index to the label name (the word spoken)\n",
    "    _y = [label_list[s] for s in y]\n",
    "    return manifold, np.array(_y)\n",
    "    \n",
    "transform = torchaudio.transforms.Spectrogram(n_fft=256)    \n",
    "manifold, y = compute_manifold(val_set, n_samples=1000, transform=transform, is_random=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b2fb9c",
   "metadata": {},
   "source": [
    "# Ordered samples UMAP projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f8d3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.plot\n",
    "umap.plot.points(manifold, labels=y, theme=\"fire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be9d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can see that there is a clear separation between the first few ordered classes\n",
    "# for a majority of the samples. Shows the spectrograms contain good features that\n",
    "# can be used as input features to a classifier model. \n",
    "\n",
    "# The Plan is to use this to fine-tune / train an image based CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece9ef94",
   "metadata": {},
   "source": [
    "# Random samples UMAP projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.plot\n",
    "manifold, y = compute_manifold(val_set, n_samples=1000, transform=transform, is_random=True)\n",
    "umap.plot.points(manifold, labels=y, theme=\"fire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need more samples to see the pattern in all of the data. \n",
    "# But there are too many samples to do this cleanly in a notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
